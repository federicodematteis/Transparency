{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:477: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "<>:484: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "<>:477: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "<>:484: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/federico/root/root-6.24.06-install/regression6.ipynb Cell 1'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/federico/root/root-6.24.06-install/regression6.ipynb#ch0000000?line=196'>197</a>\u001b[0m     transp_fill2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(transp_fill2, transp2)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/federico/root/root-6.24.06-install/regression6.ipynb#ch0000000?line=198'>199</a>\u001b[0m transp_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(transp_fill, transp_fill1)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/federico/root/root-6.24.06-install/regression6.ipynb#ch0000000?line=199'>200</a>\u001b[0m transp_train\u001b[39m.\u001b[39;49mappend(transp_fill2)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/federico/root/root-6.24.06-install/regression6.ipynb#ch0000000?line=200'>201</a>\u001b[0m \u001b[39m#transp_train=transp_fill\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/federico/root/root-6.24.06-install/regression6.ipynb#ch0000000?line=201'>202</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/federico/root/root-6.24.06-install/regression6.ipynb#ch0000000?line=202'>203</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/federico/root/root-6.24.06-install/regression6.ipynb#ch0000000?line=203'>204</a>\u001b[0m \u001b[39m#Metadata (input) for training related to each xtal\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/federico/root/root-6.24.06-install/regression6.ipynb#ch0000000?line=204'>205</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/federico/root/root-6.24.06-install/regression6.ipynb#ch0000000?line=205'>206</a>\u001b[0m \u001b[39m#-----iring23 \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/federico/root/root-6.24.06-install/regression6.ipynb#ch0000000?line=206'>207</a>\u001b[0m instLumi \u001b[39m=\u001b[39m (\u001b[39m1e-9\u001b[39m)\u001b[39m*\u001b[39mmetadata_fill\u001b[39m.\u001b[39mloc[:,\u001b[39m'\u001b[39m\u001b[39mlumi_inst\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "#baseline setup\n",
    "from cmath import exp\n",
    "from itertools import tee\n",
    "from tkinter.ttk import LabelFrame\n",
    "import matplotlib.pyplot as plt\n",
    "#import ROOT\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#libraries for general data analysis \n",
    "from pprint import pprint\n",
    "from collections import namedtuple\n",
    "import datetime\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chisquare\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.keras.backend import get_session\n",
    "\n",
    "#libraries for DNN\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, LeakyReLU, Add, Concatenate, Dot\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils  import plot_model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import seaborn as sns\n",
    "\n",
    "data_folder = ('/home/federico/root/root-6.24.06-install')\n",
    "#data_folder = ('/gwpool/users/fdematteis') #CERN server\n",
    "\n",
    "metadata = pd.read_csv(f\"{data_folder}/fill_metadata_2017_10min.csv\")\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#transparency data for a selected iRing : we will exctract the target function from this data\n",
    "\n",
    "#23\n",
    "data24=np.load(f\"{data_folder}/iRing23new.npy\")\n",
    "data24_df = pd.DataFrame(data24)\n",
    "data24_df.head()\n",
    "mean24=[]\n",
    "#25\n",
    "data25=np.load(f\"{data_folder}/iRing25new.npy\")\n",
    "data25_df = pd.DataFrame(data25)\n",
    "data25_df.head()\n",
    "mean25=[]\n",
    "#26\n",
    "data26=np.load(f\"{data_folder}/iRing26new.npy\")\n",
    "data26_df = pd.DataFrame(data26)\n",
    "data26_df.head()\n",
    "mean26=[]\n",
    "\n",
    "#compute mean transparnecy in iRings , the actual target function\n",
    "for i in range (0, len(data24_df.axes[1])):\n",
    "    #mean of colums' entries of data_df (for each \"position\" in the cristal)\n",
    "    mean24 = np.append(mean24, np.mean(data24_df[i]))\n",
    "\n",
    "for i in range (0, len(data25_df.axes[1])):\n",
    "    mean25 = np.append(mean25, np.mean(data25_df[i]))\n",
    "\n",
    "for i in range (0, len(data26_df.axes[1])):\n",
    "    mean26 = np.append(mean26, np.mean(data26_df[i]))\n",
    "\n",
    "#iring23\n",
    "\n",
    "mean24=mean24[mean24 != -1]\n",
    "metadata = metadata.iloc[:len(mean24)][mean24 != -1]\n",
    "#iring25\n",
    "mean25=mean25[mean25 != -1]\n",
    "metadata1 = metadata.iloc[:len(mean25)][mean25 != -1]\n",
    "#iring26\n",
    "mean26=mean26[mean26 != -1]\n",
    "metadata2 = metadata.iloc[:len(mean26)][mean26 != -1]\n",
    "\n",
    "#selecting metadata for fill (locking metadata to in_fill=1)\n",
    "#fill a list containing fill_num we want to use for training the DNN\n",
    "fill=metadata[\"fill_num\"].unique()\n",
    "fill = fill[fill != 0]\n",
    "\n",
    "fill1=metadata1[\"fill_num\"].unique()\n",
    "fill1 = fill1[fill1 != 0]\n",
    "\n",
    "fill2=metadata2[\"fill_num\"].unique()\n",
    "fill2 = fill2[fill2 != 0]\n",
    "\n",
    "#--------Fill excluded from iring 23\n",
    "nonsmooth = [5830, 5837, 5839, 5840, 5842, 5864, 5882, 5887, 5954, 5984, 6024, \n",
    "             6030, 6041, 6057, 6084, 6089, 6090, 6091, 6096, 6105, 6106, 6116,\n",
    "             6152, 6159, 6160, 6167, 6168, 6192, 6193, 6263, 6318,\n",
    "\n",
    "             #we exclude some fills for validation for iring 23\n",
    "             6324, 6371, 6031, 6356, 6053, 5958, 6110, 6046\n",
    "             ]\n",
    "\n",
    "#--------Fill excluded from iring 25 \n",
    "nonsmooth1 = [5830, 5837, 5839, 5840, 5842, 5864, 5882, 5883, 5887, 5954, 5980,\n",
    "              5984, 6030, 6041, 6057, 6084, 6096, 6105, 6106, 6116, 6119, 6152, \n",
    "              6159, 6160, 6167, 6168, 6170, 6171, 6192, 6261, 6262, 6263, 6279, \n",
    "              6300, 6318, 6348, 6349,\n",
    "\n",
    "              #we exclude some fills for validation for iring 25\n",
    "              6324, 6371, 6031, 6356, 6053, 5958, 6110, 6046\n",
    "              ]\n",
    "\n",
    "nonsmooth2 = [5830, 5837, 5839, 5840, 5842, 5864, 5882, 5883, 5887, 5954, 5980,\n",
    "              5984, 6030, 6041, 6057, 6084, 6096, 6105, 6106, 6116, 6119, 6152, \n",
    "              6159, 6160, 6167, 6168, 6170, 6171, 6192, 6261, 6262, 6263, 6279, \n",
    "              6300, 6318, 6348, 6349,\n",
    "\n",
    "              #we exclude some fills for validation for iring 25\n",
    "              6324, 6371, 6031, 6356, 6053, 5958, 6110, 6046\n",
    "              ]            \n",
    "#with this command we actually exclude nonsmooth fills from the \"fill\" array \n",
    "#but we are also excluding validation fills \n",
    "\n",
    "for iev in range (0, len(nonsmooth)) :\n",
    "    #print(nonsmooth[iev])\n",
    "    fill = fill[fill != nonsmooth[iev]]\n",
    "\n",
    "for iev in range (0, len(nonsmooth1)) :\n",
    "    #print(nonsmooth[iev])\n",
    "    fill1 = fill1[fill1 != nonsmooth1[iev]]\n",
    "\n",
    "\n",
    "#sbloccare quando inserisco i fill da escludere (sopra)\n",
    "for iev in range (0, len(nonsmooth2)) :\n",
    "    #print(nonsmooth[iev])\n",
    "    fill2 = fill2[fill2 != nonsmooth2[iev]]\n",
    "\n",
    "\n",
    "#locking metadata to fill_num in the fill list (we are excluding nonsmooth fills from metadata_fill)\n",
    "#metadata_fill is the actual training dataset\n",
    "metadata_fill = metadata[metadata.fill_num.isin(fill)]\n",
    "metadata_fill = metadata_fill[(metadata_fill.lumi_inst >= 0.0001*1e9) & (metadata_fill.lumi_inst <= 0.0004*1e9) & (metadata_fill.lumi_in_fill >= 0.1*1e9)]\n",
    "fill_num = metadata_fill.fill_num.unique()\n",
    "\n",
    "metadata_fill1 = metadata[metadata.fill_num.isin(fill1)]\n",
    "metadata_fill1 = metadata_fill1[(metadata_fill1.lumi_inst >= 0.0001*1e9) & (metadata_fill1.lumi_inst <= 0.0004*1e9) & (metadata_fill1.lumi_in_fill >= 0.1*1e9)]\n",
    "fill_num1 = metadata_fill1.fill_num.unique()\n",
    "\n",
    "metadata_fill2 = metadata[metadata.fill_num.isin(fill2)]\n",
    "metadata_fill2 = metadata_fill2[(metadata_fill2.lumi_inst >= 0.0001*1e9) & (metadata_fill2.lumi_inst <= 0.0004*1e9) & (metadata_fill2.lumi_in_fill >= 0.1*1e9)]\n",
    "fill_num2 = metadata_fill2.fill_num.unique()\n",
    "\n",
    "\n",
    "#Normalising the target function to the initial value of each fill\n",
    "#We normalise because we consider Transparency to be 1 at the beginning of each fill\n",
    "#Our target is the predicted Transparency in a single fill\n",
    "\n",
    "#Instancing arrays\n",
    "transp_fill = []\n",
    "transp_fill1 = []\n",
    "transp_fill2 = []\n",
    "lumi_inst_0 = []\n",
    "lumi_int_0 = []\n",
    "\n",
    "\n",
    "#Now we calculate the mean transparency in iring, averaging trasparency data over positions \n",
    "#in the xtal; Transparency data are measured in various positions in the xtal,\n",
    "#Average value of Transparency is the actual target function in this machine learning problem.\n",
    "\n",
    "# iring23\n",
    "for k in fill_num:\n",
    "#transparency for selected fills\n",
    "    df = metadata_fill[metadata_fill.fill_num == k]\n",
    "    transp = [mean24[i] for i in df.index.values]\n",
    "    #transp has the size of the dataframe locked to fill_num = k\n",
    "    transp = transp/transp[0]\n",
    "    transp_fill = np.append(transp_fill, transp)\n",
    "    \n",
    "    a = np.empty(np.size(transp))\n",
    "    b = np.empty(np.size(transp))\n",
    "    a.fill(df['lumi_inst'].iloc[0])\n",
    "    b.fill(df['lumi_int'].iloc[0])\n",
    "    \n",
    "    lumi_inst_0 = np.append(lumi_inst_0, a)\n",
    "    lumi_int_0 = np.append(lumi_int_0, b)\n",
    "    #in transp_fill ci sono i dati di trasparenza normalizzata per ogni fill;\n",
    "\n",
    "# iring25\n",
    "for k in fill_num1:\n",
    "    df1 = metadata_fill1[metadata_fill1.fill_num == k]\n",
    "    transp1 = [mean25[i] for i in df1.index.values]\n",
    "    transp1 = transp1/transp1[0]\n",
    "    transp_fill1 = np.append(transp_fill1, transp1)\n",
    "\n",
    "for k in fill_num2:\n",
    "    df2 = metadata_fill2[metadata_fill1.fill_num == k]\n",
    "    transp2 = [mean26[i] for i in df2.index.values]\n",
    "    transp2 = transp2/transp2[0]\n",
    "    transp_fill2 = np.append(transp_fill2, transp2)\n",
    "\n",
    "transp_train = np.append(transp_fill, transp_fill1)\n",
    "transp_train.append(transp_fill2)\n",
    "#transp_train=transp_fill\n",
    "\n",
    "\n",
    "#Metadata (input) for training related to each xtal\n",
    "\n",
    "#-----iring23 \n",
    "instLumi = (1e-9)*metadata_fill.loc[:,'lumi_inst']\n",
    "intLumiLHC = (1e-9)*metadata_fill.loc[:,'lumi_int']\n",
    "infillLumi = (1e-9)*metadata_fill.loc[:,'lumi_in_fill']\n",
    "lastfillLumi = (1e-9)*metadata_fill.loc[:,'lumi_last_fill']\n",
    "filltime = (1e-9)*metadata_fill.loc[:,'time_in_fill']\n",
    "lastpointLumi = (1e-9)*metadata_fill.loc[:, 'lumi_since_last_point']\n",
    "true_time = (1e-9)*metadata_fill.loc[:, 'time']\n",
    "\n",
    "ring_index = np.zeros(len(metadata_fill))\n",
    "for i in range (0, len(ring_index)):\n",
    "    ring_index[i] = 23\n",
    "\n",
    "#-----iring25\n",
    "instLumi1 = (1e-9)*metadata_fill1.loc[:,'lumi_inst']\n",
    "intLumiLHC1 = (1e-9)*metadata_fill1.loc[:,'lumi_int']\n",
    "infillLumi1 = (1e-9)*metadata_fill1.loc[:,'lumi_in_fill']\n",
    "lastfillLumi1 = (1e-9)*metadata_fill1.loc[:,'lumi_last_fill']\n",
    "filltime1 = (1e-9)*metadata_fill1.loc[:,'time_in_fill']\n",
    "lastpointLumi1 = (1e-9)*metadata_fill1.loc[:, 'lumi_since_last_point']\n",
    "true_time1 = (1e-9)*metadata_fill1.loc[:, 'time']\n",
    "\n",
    "ring_index1 = np.zeros(len(metadata_fill1))\n",
    "for i in range (0, len(ring_index1)):\n",
    "    ring_index1[i] = 25\n",
    "\n",
    "#------iring 26\n",
    "instLumi2 = (1e-9)*metadata_fill2.loc[:,'lumi_inst']\n",
    "intLumiLHC2 = (1e-9)*metadata_fill2.loc[:,'lumi_int']\n",
    "infillLumi2 = (1e-9)*metadata_fill2.loc[:,'lumi_in_fill']\n",
    "lastfillLumi2 = (1e-9)*metadata_fill2.loc[:,'lumi_last_fill']\n",
    "filltime2 = (1e-9)*metadata_fill2.loc[:,'time_in_fill']\n",
    "lastpointLumi2 = (1e-9)*metadata_fill2.loc[:, 'lumi_since_last_point']\n",
    "true_time2 = (1e-9)*metadata_fill2.loc[:, 'time']\n",
    "\n",
    "ring_index2 = np.zeros(len(metadata_fill2))\n",
    "for i in range (0, len(ring_index2)):\n",
    "    ring_index2[i] = 26\n",
    "\n",
    "\n",
    "#now i have to merge transparency data and luminosity metadata into single objects\n",
    "\n",
    "#metadata merge\n",
    "merged_instLumi = np.append(instLumi, instLumi1) \n",
    "merged_instLumi.append(instLumi2)\n",
    "merged_intLumiLHC = np.append(intLumiLHC, intLumiLHC1)   \n",
    "merged_intLumiLHC.append(intLumiLHC2)\n",
    "merged_infillLumi = np.append(infillLumi, infillLumi1)\n",
    "merged_infillLumi.append(infillLumi2)\n",
    "merged_lastfillLumi = np.append(lastfillLumi, lastfillLumi1)\n",
    "merged_lastfillLumi.append(lastfillLumi2)\n",
    "merged_filltime = np.append(filltime, filltime1)\n",
    "merged_filltime.append(filltime2)\n",
    "merged_ring_index = np.append(ring_index, ring_index1)\n",
    "merged_ring_index.append(ring_index2)\n",
    "\n",
    "#transparency merge \n",
    "\n",
    "#all_inputs_train=np.stack((instLumi, infillLumi, intLumiLHC, filltime, ring_index, lastfillLumi), axis=-1)\n",
    "\n",
    "#Validation dataset\n",
    "#fill usati per il test\n",
    "#filltest = [6324, 6371, 6031, 6356, 6053, 5958, 6110, 6046]\n",
    "filltest = [5958, 6031, 6046, 6053, 6110, 6324, 6356, 6371]\n",
    "metadata_test = metadata[metadata.fill_num.isin(filltest)]\n",
    "\n",
    "metadata_test = metadata_test[(metadata_test.lumi_inst >= 0.0001*1e9) & (metadata_test.lumi_inst <= 0.0004*1e9) & (metadata_test.lumi_in_fill >= 0.1*1e9)]\n",
    "#estraggo transparency per il test\n",
    "transp_test = mean24[metadata_test.index.values[0]:metadata_test.index.values[0]+len(metadata_test.axes[0])]\n",
    "fill_num_test = metadata_test.fill_num.unique()\n",
    "\n",
    "#normalizzo i dati di trasparenza per il test\n",
    "transp_test_final=[]\n",
    "\n",
    "for k in fill_num_test:\n",
    "    df_test = metadata_test[metadata_test.fill_num == k]\n",
    "    #sto scegliendo i dati di trasparenza dall'iRing 23 che quì è indicato con 24\n",
    "    transp_test = [mean24[i] for i in df_test.index.values]\n",
    "    transp_test = transp_test/transp_test[0]\n",
    "    transp_test_final = np.append(transp_test_final, transp_test)\n",
    "\n",
    "#print(transp_test_final)\n",
    "\n",
    "\n",
    "#Ora devo preparare i metadati di validation usando metadata_test\n",
    "\n",
    "instLumi_test = (1e-9)*metadata_test.loc[:,'lumi_inst']\n",
    "intLumiLHC_test = (1e-9)*metadata_test.loc[:,'lumi_int']\n",
    "infillLumi_test = (1e-9)*metadata_test.loc[:,'lumi_in_fill']\n",
    "lastfillLumi_test = (1e-9)*metadata_test.loc[:,'lumi_last_fill']\n",
    "filltime_test = (1e-9)*metadata_test.loc[:,'time_in_fill']\n",
    "lastpointLumi_test = (1e-9)*metadata_test.loc[:, 'lumi_since_last_point']\n",
    "true_time_test = (1e-9)*metadata_test.loc[:, 'time']\n",
    "\n",
    "\n",
    "ring_index_test = np.zeros(len(metadata_test))\n",
    "for iev in range (0, len(metadata_test)):\n",
    "    ring_index_test[iev] = 23\n",
    "\n",
    "#all_inputs_test=np.stack((instLumi_test, infillLumi_test, intLumiLHC_test, filltime_test, ring_index_test, lastfillLumi_test, Norm_time_in_fill_test), axis=-1)\n",
    "\n",
    "# Machine learning  ─=≡Σ(([ ⊐•̀⌂•́]⊐\n",
    "#defining a custom loss function\n",
    "import keras.backend as K\n",
    "\n",
    "#validation time_in_fill normalizzato\n",
    "Norm_time_in_fill_test=[]\n",
    "Norm_time_in_fill_prov_test=[]\n",
    "exp_time_in_fill_prov=[]\n",
    "weightstest=[]\n",
    "for k in fill_num_test:\n",
    "    dftest = metadata_test[metadata_test.fill_num == k]\n",
    "    \n",
    "    #time_in_fill normalizzato per ogni fill, rispetto all'istante iniziale\n",
    "    Norm_time_in_fill_prov_test = dftest.loc[:,'time_in_fill']\n",
    "    c=dftest['time_in_fill'].iloc[0]\n",
    "    Norm_time_in_fill_prov_test = Norm_time_in_fill_prov_test/c\n",
    "    Norm_time_in_fill_test=np.append(Norm_time_in_fill_test,Norm_time_in_fill_prov_test)\n",
    "    weightstest.append((0.6/len(fill_num_test)))\n",
    "    for i in range (1, len(dftest)):\n",
    "        #weights.append(0)\n",
    "        weightstest.append((1-0.6)/(len(transp_train)-len(fill_num_test)))\n",
    "    #print('time in fill normalizzato')\n",
    "    #print(Norm_time_in_fill_test)\n",
    "\n",
    "all_inputs_test=np.stack((instLumi_test, infillLumi_test, intLumiLHC_test, filltime_test, ring_index_test, lastfillLumi_test), axis=-1)#, Norm_time_in_fill_test), axis=-1)\n",
    "\n",
    "\n",
    "#TRAIN\n",
    "Norm_time_in_fill_train=[]\n",
    "Norm_time_in_fill_prov_train=[]\n",
    "weightstrain=[]\n",
    "\n",
    "for k in fill_num:\n",
    "    dftrain = metadata_fill[metadata_fill.fill_num == k]\n",
    "    \n",
    "    #time_in_fill normalizzato per ogni fill, rispetto all'istante iniziale\n",
    "    Norm_time_in_fill_prov_train = dftrain.loc[:,'time_in_fill']\n",
    "    Norm_time_in_fill_prov_train = Norm_time_in_fill_prov_train/dftrain['time_in_fill'].iloc[0]\n",
    "    Norm_time_in_fill_train=np.append(Norm_time_in_fill_train,Norm_time_in_fill_prov_train)\n",
    "    #weights.append(1)\n",
    "    weightstrain.append((0.01/len(fill_num)))\n",
    "    for i in range (1, len(dftrain)):\n",
    "        #weights.append(0)\n",
    "        weightstrain.append((1-0.01)/(len(transp_fill)-len(fill_num)))\n",
    "    #print('time in fill normalizzato')\n",
    "    #print(Norm_time_in_fill_train)\n",
    "    #print(len(Norm_time_in_fill))\n",
    "\n",
    "    \n",
    "Norm_time_in_fill_train1=[]\n",
    "Norm_time_in_fill_prov_train1=[]\n",
    "weightstrain1=[]\n",
    "\n",
    "#Normalizzo time in fill per xtal 25 (iring25)\n",
    "for k in fill_num1:\n",
    "    dftrain = metadata_fill1[metadata_fill1.fill_num == k]\n",
    "    \n",
    "    #time_in_fill normalizzato per ogni fill, rispetto all'istante iniziale\n",
    "    Norm_time_in_fill_prov_train1 = dftrain.loc[:,'time_in_fill']\n",
    "    Norm_time_in_fill_prov_train1 = Norm_time_in_fill_prov_train1/dftrain['time_in_fill'].iloc[0]\n",
    "    Norm_time_in_fill_train1=np.append(Norm_time_in_fill_train1,Norm_time_in_fill_prov_train1)\n",
    "    #weights.append(1)\n",
    "    weightstrain.append((0.01/len(fill_num1)))\n",
    "    for i in range (1, len(dftrain)):\n",
    "        #weights.append(0)\n",
    "        weightstrain1.append((1-0.01)/(len(transp_fill1)-len(fill_num1)))\n",
    "\n",
    "Norm_time_in_fill_train2=[]\n",
    "Norm_time_in_fill_prov_train2=[]\n",
    "weightstrain2=[]\n",
    "\n",
    "for k in fill_num2:\n",
    "    dftrain = metadata_fill2[metadata_fill2.fill_num == k]\n",
    "    #time_in_fill normalizzato per ogni fill, rispetto all'istante iniziale\n",
    "    Norm_time_in_fill_prov_train2 = dftrain.loc[:,'time_in_fill']\n",
    "    Norm_time_in_fill_prov_train2 = Norm_time_in_fill_prov_train2/dftrain['time_in_fill'].iloc[0]\n",
    "    Norm_time_in_fill_train2=np.append(Norm_time_in_fill_train2,Norm_time_in_fill_prov_train2)\n",
    "    #weights.append(1)\n",
    "    weightstrain.append((0.01/len(fill_num2)))\n",
    "    for i in range (1, len(dftrain)):\n",
    "        #weights.append(0)\n",
    "        weightstrain2.append((1-0.01)/(len(transp_fill2)-len(fill_num2)))\n",
    "\n",
    "\n",
    "\n",
    "merged_norm_time_in_fill=[]\n",
    "#devo fare un append dei time in fill normalizzati per i due cristalli 23-25\n",
    "\n",
    "merged_norm_time_in_fill = np.append(Norm_time_in_fill_train,Norm_time_in_fill_train1)\n",
    "merged_norm_time_in_fill.append(Norm_time_in_fill_prov_train2)\n",
    "#append dei pesi virtuali della loss\n",
    "\n",
    "#questo era weights ma non vogliamo questo \n",
    "#weights=[]\n",
    "#weights=np.append(weightstrain,weightstrain1)\n",
    "#redefine virtual weights\n",
    "weights=[]\n",
    "\n",
    "for k in range (0, len(merged_norm_time_in_fill)-1):\n",
    "    if merged_norm_time_in_fill[k] == 1. :\n",
    "        weight = 0.5/(len(fill_num2)+len(fill_num)+len(fill_num1))\n",
    "        weights.append(weight)\n",
    "    else:\n",
    "        weight = (1-0.5)/ (len(fill_num2)+len(merged_norm_time_in_fill)-(len(fill_num)+len(fill_num1)))\n",
    "        weights.append(weight)\n",
    "print('pesi virtuali per la loss function')\n",
    "print(len(weights))\n",
    "\n",
    "\n",
    "#devo inserire i dati di xtal 25 nella lista di array \"all_inputs_train\"\n",
    "#all_inputs_train=np.stack((instLumi, infillLumi, intLumiLHC, filltime, ring_index, lastfillLumi, Norm_time_in_fill_train), axis=-1)\n",
    "\n",
    "all_inputs_train=np.stack((merged_instLumi, merged_infillLumi, merged_intLumiLHC, merged_filltime, merged_ring_index, merged_lastfillLumi), axis=-1)#, merged_norm_time_in_fill), axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "print(len(all_inputs_test))\n",
    "\n",
    "#print('tempo in fill norm test')\n",
    "#print(len(Norm_time_in_fill_test))\n",
    "#print('tempo in fill norm train')\n",
    "#print(len(Norm_time_in_fill_train))\n",
    "#provo a definire un vettore con dentro gli esponenziali:\n",
    "    # for k in range (0,len(df)):\n",
    "    #     exp_time_in_fill_prov = exp(Norm_time_in_fill_prov_test[k] -c)\n",
    "    #     print(exp_time_in_fill_prov)\n",
    "\n",
    "    # exp_time_in_fill = np.append(exp_time_in_fill, exp_time_in_fill_prov)\n",
    "\n",
    "from math import e\n",
    "proof=e**0\n",
    "#print('esponenziale di 0')\n",
    "#print(proof)\n",
    "plt.plot(Norm_time_in_fill_test, (1+10*e**(-1000*(Norm_time_in_fill_test-1))), \".b\", markersize=3, linewidth=0.75)\n",
    "plt.xlabel(\"Time_in_fill\")\n",
    "plt.ylabel(\"exp(-(time_in_fill-1))\")\n",
    "plt.tick_params(labelsize=7)\n",
    "plt.title('fattore moltiplicativo della loss function')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.show()\n",
    "custom_delta=[]\n",
    "\n",
    "#generic version\n",
    "# def custom_loss(y_true,y_pred,inputs):\n",
    "#     def loss (y_true,y_pred):       \n",
    "#         return K.square(y_true-y_pred) * 10*e**(-1000*(inputs(7,)-1))\n",
    "#     return loss\n",
    "\n",
    "#train oriented version\n",
    "# def delta(transp_training,transp_predicted_train):\n",
    "#     return ((transp_training-transp_predicted_train)**2)/tf.cast(len(transp_predicted_train), tf.float32) * (1+ 10*e**(-1000*(Norm_time_in_fill_train-1)))\n",
    "\n",
    "#train oriented with sample weighting\n",
    "def delta_train(transp_training,transp_predicted_train):\n",
    "    loss = K.square(transp_training-transp_predicted_train)\n",
    "    loss=loss*weights\n",
    "    loss=K.sum(loss, axis=1)\n",
    "    return loss\n",
    "\n",
    "#test loss obtained with sample weighting\n",
    "def delta_test(transp_test_final,transp_predicted_test):\n",
    "    loss = K.square(transp_test_final-transp_predicted_test)\n",
    "    loss=loss*weightstest\n",
    "    loss=K.sum(loss, axis=1)\n",
    "    return loss\n",
    "\n",
    "#custom exponential MSE depending on input layer (work in progress)\n",
    "def custom_loss (inputs):\n",
    "    def delta (y_true,y_pred):\n",
    "        loss = K.square(y_true-y_pred) * 10*e**(-1000(inputs(7,)-1))\n",
    "        loss=K.sum(loss, axis=1)\n",
    "        return loss\n",
    "    return delta\n",
    "\n",
    "#custom exponential MSE depending on normalised time in fill\n",
    "def delta_mse (transp_training,transp_predicted_train):\n",
    "    delta  = K.square(transp_training-transp_predicted_train) * 10*e**(-1000(Norm_time_in_fill_train-1))\n",
    "    delta = K.sum(delta, axis=1)\n",
    "    return delta\n",
    "\n",
    "#DNN structure\n",
    "inputs = Input(shape=(6,))\n",
    "hidden1 = Dense(256, activation='leaky_relu', kernel_regularizer=regularizers.l1_l2(l1=1e-9, l2=1e-10), bias_regularizer=regularizers.l2(1e-10), activity_regularizer=regularizers.l2(1e-10))(inputs)\n",
    "drop1=Dropout(0.2)(hidden1)\n",
    "hidden2 = Dense(128, activation='leaky_relu', kernel_regularizer=regularizers.l1_l2(l1=1e-9, l2=1e-10), bias_regularizer=regularizers.l2(1e-10), activity_regularizer=regularizers.l2(1e-10))(drop1)\n",
    "drop2=Dropout(0.2)(hidden2)\n",
    "hidden3 = Dense(64, activation='leaky_relu', kernel_regularizer=regularizers.l1_l2(l1=1e-9, l2=1e-10), bias_regularizer=regularizers.l2(1e-10), activity_regularizer=regularizers.l2(1e-10))(drop2)\n",
    "outputs = Dense(1) (hidden3)\n",
    "\n",
    "#model checkpoint and early stopping\n",
    "filepath = \"/home/federico/root/root-6.24.06-install/weights\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_mean_squared_error', verbose=0, save_best_only=True, mode='min')\n",
    "callback_list=[checkpoint]\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=50, verbose=0, restore_best_weights= True)\n",
    "\n",
    "\n",
    "model = Model ( inputs=inputs, outputs=outputs )\n",
    "\n",
    "#model.add_loss( custom_loss( ,outputs, inputs) )\n",
    "\n",
    "model.compile(loss = delta_train, optimizer='adam', metrics=[delta_train, 'MSE'])\n",
    "\n",
    "#write the summary of the network\n",
    "model.summary()\n",
    "\n",
    "#plot the network\n",
    "plot_model(\n",
    "    model,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    ")\n",
    "\n",
    "all_inputs_training   = all_inputs_train\n",
    "all_inputs_validation = all_inputs_test\n",
    "transp_training   = transp_train\n",
    "transp_validation = transp_test_final\n",
    "#print(all_inputs_validation)\n",
    "print('forma dellinput prima di model.fit')\n",
    "#print(len(inputs(7)))\n",
    "#print(model.layers[0,:])\n",
    "history = model.fit( all_inputs_training, transp_training, validation_data = (all_inputs_validation,transp_test_final), epochs=300, verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "#plot the training loss\n",
    "plt.plot( history.history[\"loss\"], label = 'train' )\n",
    "plt.plot( history.history[\"val_loss\"], label = 'validation' )#devo inserire momenti senza radiazione per il train; non mi interessa usarli nel test\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "transp_predicted_validation = model.predict(all_inputs_validation)\n",
    "transp_predicted_train = model.predict(all_inputs_train)\n",
    "\n",
    "prediction_single_fill=[]\n",
    "\n",
    "\n",
    "#plot Transparency vs abs time of validation\n",
    "plt.plot(metadata_test.time, transp_validation, \".b-\", markersize=3, linewidth=0.75, label=\"measured\")\n",
    "plt.plot(metadata_test.time, transp_predicted_validation, \".r-\", markersize=3, linewidth=0.75, label=\"predicted\")\n",
    "plt.xlabel(\"absolute time\")\n",
    "plt.ylabel(\"mean transparency\")\n",
    "plt.tick_params(labelsize=7)\n",
    "plt.title(f\"fill {fill_num_test}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"Plot generated using Matplotlib.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
